---
title: "Homework 3"
author: "Luis Noguera"
date: "5/31/2020"
output: 
  pdf_document:
    latex_engine: xelatex

---

# Initial Set Up
```{r warning=FALSE, echo=TRUE, eval= TRUE, message = F}

# Importing libraries and set up the work environment
library(knitr)
library(readxl)
library(tidyverse)
library(janitor)
library(rpart)
library(tidymodels)
library(nnet)

knitr::opts_chunk$set(cache = TRUE, 
                      warning = FALSE, 
                      echo = TRUE,
                      message = FALSE,
                      dpi = 180, 
                      fig.width = 6, 
                      fig.height = 4)


theme_set(theme_classic())

```


# Question 1
## Math heavy!

# Question 2
## Math heavy!

# Question 3
## Math heavy!

# Question 4
## Math heavy!


# Question 5

**Suppose there are several outcome variables {y1,y2,···,yM} associated with a commonset of predictor variablesx={x1,x2,· · ·,xn}. One could train separate single output neuralnetworks for each outcome ymor train a single network with multiple outputs, one for each ym.What are the relative advantages/disadvantages of these two respective approaches. In what situations would one expect each to be better than the other.**



Training Multiple Networks:

*Advantages*

- It is possible to fine-tune multiple parameters in each network to estimate a specific response accurately. 
- If the response variables are unrelated to one an another than training multiple networks is more beneficial than using only one model that tries to learn and estimate multiple outcomes. 

*Disadvantages*

- A lot more computational time and power would be required when traning multiple newtorks and parameters. 
- More parameters included on the training set, may overfit the model when trying to estimate on the testing or unseen data.


In summary, is appropiate to train multiple models for each variable when outcome variables are unrelated and computational power and time are not of concern. however, if the response variables are related training one network model to predict multiple variables could be beneficial; if the risk of overfitting exist on the training set due to small amount of data an also benefical approach is to used multi-task learning. 



# Question 6 

**Describe K—fold cross-validation. What is it used for. What are the advantages/disadvantages of using more folds (increasing K). When does cross—validation estimate the performance of the actual predicting function being used.**


K-Folds is a method for splitting the training data into multiple groups or subsets, each group has a training, testing set and K represents the total number of groups/subsets. The statistical model is trained K folds, leaving out a different random set for evaluation. Once the split and fitting into the multiple sets is completed, the model's prediction error is averaged over each K fold. 

Cross-calidation is used for evaluation purposes, specially when multiple models are evaluated. This technique is particularly advantageous in small there is not enough data to have only one set for validation and training purposes. In most cases, the statistical model chosen is the one that holds the lowest average error from the K-folds. Once the model has been chosen is usually trained on the whole dataset. 


*Advantages*

- More of the data available can be used for traning purposes as it gets divided into multile groups. This provides an advantage of lower bias in the model.
- Second, the cross-validation estimation provides a more accurate and less bias representation of the expected prediction error for the final use case. 

*Disadvantages*

- As the number of K-folds increases the computational power required for the fitting the model on each fold also increases. 
- A higher number of cross-validation folds includes a higher number of observations in the dataset, making the training datasets very similar with one another, therefore prompt to overfitting and not accurate on new unseen data. 


As opposed to estimating the performance on the actual predction function, K-fold cross validation measures the error on the modelling and fitting procedure; testing how good or bad are different hyperparameter tunning for example. If each of the parameters learned in all the evaluation folds are identical then K-fold cross validation would estimate the performance of the actual prediction function, not possible in practice. 

# Question 7
## Math heavy!

# Question 8

**Consider near neighbor regression.(a) What are the advantages and disadvantages of using a large neighborhood size M ?(b) Describe the advantages and disadvantages of standardizing each of the variables to have the same variance before training.(c) Through a small simulated example, show that standardization in (b) may not always be a good idea.**


*Advantages*

- Increasing the number of K in nearest neighbor regression could effectively make the model more precise, identifying patterns and relationships across many different dimensions. 

*Disadvantage*

- Using a large neighbor size M, could lead to overfitting the evaluation model as the same relationships in data may not be repeated across the evaluation or new data. 

*Advantages of Standardizing*

- With different measurement scales across multiple variables, one variable could have a much higher influence in the model then others if it's not standardized. Generally, variables with high variance can have major influence over the model so it is important to standardize in this case. 


*Disadvantages of Standardizing*

- Categorical variables would need to be transformed into dummy variables in order to standardize the data with all the features, adding one more step to the pre-processing step. 
- Interpreting coefficients in linear models is misleading as the original data has been manipulated. 
- When converting a continous variable to categorical to capture different groups, the distance and variance of the variables it is of importance. 


Standardization in linear model is not a good idea as it does not change or affect the model performance in any form. Here an example and evaluation on the training set:


```{r}
set.seed(415)
train <- data.frame(
  X1 = sample(40:2000, 100, replace = T),
  X2 = sample(2:10, 100, replace = T)
)

train$y <- with(train,2*X1 + 3.4*X2 + rnorm(100,sd=60))

# Linear Model Fit not Standardized
fit_ns <- lm(y ~ X1 + X2, data = train)
summary(fit_ns)


```


```{r}


train_scaled <- as.data.frame(scale(train, center = T, scale = T))

# Linear Model Fit Standardized
fit_standard <- lm(y ~ X1 + X2, data = train_scaled)
summary(fit_standard)


```

As shown in the previous summaries, both summaries show the same Adjusted R-squared and different coefficients, standardizing the data is not always a good idea as it statistical model be built and purppse to be used. 



# Question 9


**Spam Email. The data setsspam_stats315B_train.csv, spam_stats315B_test.csvand documentation for this problem are the same as in Homework 2 and can be found in theclass web page. You need first to standardize predictors and choose all the weights startingvalues at random in the interval [-0.5, 0.5].**


# 9.0 Data Import
## TBC!
Loading the data files, assigning the header names and counting to check fro class imabalance.

## Data Import 

```{r}

spam_train <- read_csv("spam_stats315B_train.csv", 
    col_names = FALSE)

spam_test <- read_csv("spam_stats315B_test.csv", 
    col_names = FALSE)

header_spam<-c("make", "address", "all", "3d", "our", "over", "remove","internet","order", "mail", "receive", "will","people", "report", "addresses","free", "business","email", "you", "credit", "your", "font","000","money","hp", "hpl", "george", "650", "lab", "labs","telnet", "857", "data", "415", "85", "technology", "1999","parts","pm", "direct", "cs", "meeting", "original", "project","re","edu", "table", "conference", ";", "(", "[", "!", "$", "#","CAPAVE", "CAPMAX", "CAPTOT","type")

colnames(spam_train) <- header_spam
colnames(spam_test) <- header_spam

# Checking if there is class imbalance in the response. 
spam_train %>% 
  count(type)


```




```{r}

# Important Functions

pct_accuracy <- function(y_hat, y_test, threshold){
   y_hat[y_hat >  threshold] <- 1
  y_hat[y_hat <= threshold] <- 0
  correct <- y_hat == y_test
  pct_accurate <- sum(correct)/length(correct)
  return(pct_accurate)
}




#returns missclasification rates for overall and each class for a given threshold


get_misclassification_rates <- function(model, threshold){
  
  y_hat <- predict(model, test_scaled_X)
  y_hat[y_hat >  threshold] <- 1
  y_hat[y_hat <= threshold] <- 0
    
  
  correct <- y_hat == y_test
  correct_spam    <- correct[y_test == 1]
  correct_nonspam <- correct[y_test == 0]
  
  misclassification_rate <- 1 - sum(correct)/length(correct)
  spam_misclassification_rate <- 1 - sum(correct_spam)/length(correct_spam)  
  nonspam_misclassification_rate <- 1 - sum(correct_nonspam)/length(correct_nonspam)
  
  return(c(
    misclassification_rate, 
    spam_misclassification_rate, 
    nonspam_misclassification_rate)
  )
}



```




## Pre-Processing

```{r}
set.seed(415)
# Scaling Data using Tidymodels
spam_train_rec <- recipe(type ~ ., data = spam_train) %>%
  step_scale(all_predictors()) %>%
  step_zv(all_predictors()) %>%
  prep(retain = TRUE)


spam_train_proc <- bake(spam_train_rec, new_data = spam_train) %>% select(-type)
spam_test_proc <- bake(spam_train_rec, new_data = spam_test) %>% select(-type)

y_test <- spam_test_proc %>% select(type)
y_train <- spam_train_proc %>% select(type)

```


**(a) Fit on the training set one hidden layer neural networks with 1, 2,..., 10 hidden units and different sets of starting values for the weights (obtain in this way one model for each number of units). Which structural model performs best at classifying on the test set?**


## To be debugged!

```{r}

# Parameters
num_neurons <- seq(1:10)
num_reps <- 5
wt_rang = 0.5
threshold <- .5
accuracies <- c()



for(size in num_neurons){
  sum_accuracy <- 0
    
  #average accuracy over num_reps random initializations
  for(i in c(1:num_reps)){
    
    set.seed(415)
    
    model <- nnet(
        spam_train_proc, y_train, size=num_neurons[size],
        linout = FALSE,  softmax = FALSE,
        censored = FALSE, skip = FALSE, rang = wt_rang, decay = 0,
        maxit = 100, Hess = FALSE, trace = FALSE
    )
    y_hat <- predict(model, spam_test_proc)
    sum_accuracy <- sum_accuracy + pct_accuracy(y_hat, y_test, threshold)
  }
  
  accuracies <- c(accuracies, sum_accuracy/num_reps)
}



#get the num_neurons corresponding with model that produced highest average accuracy
best_performing_idx <- which.max(accuracies)
best_performing_num_neurons <- num_neurons[best_performing_idx]
best2_performing_idx <- which.max(accuracies[accuracies!=max(accuracies)])
best2_performing_num_neurons <- num_neurons[best2_performing_idx]



library(scales)
#Report output
cat("Best performing number of hidden layer neurons: ", 
    best_performing_num_neurons, "\n",
    "Accuracy: ",
    scales::percent(accuracies[best_performing_idx]),"\n")

cat("2nd best performing number of hidden layer neurons: ", 
    best2_performing_num_neurons, "\n",
    "Accuracy: ",
    scales::percent(accuracies[best2_performing_idx]),"\n")


```

